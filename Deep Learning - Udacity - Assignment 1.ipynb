{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download: notMNIST_large.tar.gz\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified .\\notMNIST_large.tar.gz\n",
      "Attempting to download: notMNIST_small.tar.gz\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified .\\notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '.' # Change me to store data elsewhere\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \n",
    "  \"\"\"\n",
    "  A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.  \n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\notMNIST_large already present - Skipping extraction of .\\notMNIST_large.tar.gz.\n",
      "['.\\\\notMNIST_large\\\\A', '.\\\\notMNIST_large\\\\B', '.\\\\notMNIST_large\\\\C', '.\\\\notMNIST_large\\\\D', '.\\\\notMNIST_large\\\\E', '.\\\\notMNIST_large\\\\F', '.\\\\notMNIST_large\\\\G', '.\\\\notMNIST_large\\\\H', '.\\\\notMNIST_large\\\\I', '.\\\\notMNIST_large\\\\J']\n",
      ".\\notMNIST_small already present - Skipping extraction of .\\notMNIST_small.tar.gz.\n",
      "['.\\\\notMNIST_small\\\\A', '.\\\\notMNIST_small\\\\B', '.\\\\notMNIST_small\\\\C', '.\\\\notMNIST_small\\\\D', '.\\\\notMNIST_small\\\\E', '.\\\\notMNIST_small\\\\F', '.\\\\notMNIST_small\\\\G', '.\\\\notMNIST_small\\\\H', '.\\\\notMNIST_small\\\\I', '.\\\\notMNIST_small\\\\J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACRUlEQVR4nG2STUiUYRDH//O8z368\nvvu9qGyZH7upaaYb9GEYlVFRknTJsFsWHTt1rEvnunSIQKhDF6EuBR6MwkzE0CQJKiFF11owd9Vd\nM0133/eZDtruYs1pmP/8/swwA+TCZRB0GyhfKUi9zpbwSNn7KWJsD4Ezj4d4+tvtCvEfreH+GGcs\njt/Nu+XbxO9qlmB/Sd0/qA0XlVLMnJ6/k5tkq0tkz3fNKwLYa2gV22wJh9itWRYYc5fLcsiW6HLW\nGUzmIni35yC4UCQzsj+saO3N+DipYNVplV+f4K58Ns4WD/uv9/1kjt2r3YQ2yeLETNQkNbA8s6wB\npd7yvC1rqQtXVyRNJAID/SMxOE85vIpypLzkM3hpcHhBvf5RzDBaG3IkWc3NmkBqJQ271S9ZFe+I\nh7S/trvML0BqcvirWF9K9whYnS1rNgASIOG5doRJD54ITB2eMU4u+bTMlbFpABLE/r1NToKokY3R\nwY6AYSfQgcjiIjGgaU3dSVaKmVmtWhvMzCY/DUtAgOjY2Q0wccYC67BDKQD1uhMQZLoDJT4I0yRO\nILH6fZ0FC+j1RSDJjrZmMjj7cK79ecyfOqr37uuotVxVNz4kCLL+5ifO8pNzERR54LOHg/Zbr2bZ\n+tgDErqjuoZlKrYQx/qKXM5OpzIPRn0QO5NdmnA3ttpUenB2coMUm8zEWvrlrwX2tFWawt8ZRppG\ne9XWhZkVJoYcq7aIuUfi7bvS4y8eJQu/zdeth/qin+OQoUqbSycqFPWgLDec7e4/wqfzL3yRo74A\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename='notMNIST_small/A/MDEtMDEtMDAudHRm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling .\\notMNIST_large\\A.pickle.\n",
      ".\\notMNIST_large\\A\n",
      "Could not read: .\\notMNIST_large\\A\\RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png : cannot identify image file '.\\\\notMNIST_large\\\\A\\\\RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png' - it's ok, skipping.\n",
      "Could not read: .\\notMNIST_large\\A\\SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png : cannot identify image file '.\\\\notMNIST_large\\\\A\\\\SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png' - it's ok, skipping.\n",
      "Could not read: .\\notMNIST_large\\A\\Um9tYW5hIEJvbGQucGZi.png : cannot identify image file '.\\\\notMNIST_large\\\\A\\\\Um9tYW5hIEJvbGQucGZi.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52909, 28, 28)\n",
      "Mean: -0.12825\n",
      "Standard deviation: 0.443121\n",
      "Pickling .\\notMNIST_large\\B.pickle.\n",
      ".\\notMNIST_large\\B\n",
      "Could not read: .\\notMNIST_large\\B\\TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png : cannot identify image file '.\\\\notMNIST_large\\\\B\\\\TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.00756303\n",
      "Standard deviation: 0.454491\n",
      "Pickling .\\notMNIST_large\\C.pickle.\n",
      ".\\notMNIST_large\\C\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.142258\n",
      "Standard deviation: 0.439806\n",
      "Pickling .\\notMNIST_large\\D.pickle.\n",
      ".\\notMNIST_large\\D\n",
      "Could not read: .\\notMNIST_large\\D\\VHJhbnNpdCBCb2xkLnR0Zg==.png : cannot identify image file '.\\\\notMNIST_large\\\\D\\\\VHJhbnNpdCBCb2xkLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.0573678\n",
      "Standard deviation: 0.455648\n",
      "Pickling .\\notMNIST_large\\E.pickle.\n",
      ".\\notMNIST_large\\E\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.069899\n",
      "Standard deviation: 0.452942\n",
      "Pickling .\\notMNIST_large\\F.pickle.\n",
      ".\\notMNIST_large\\F\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.125583\n",
      "Standard deviation: 0.44709\n",
      "Pickling .\\notMNIST_large\\G.pickle.\n",
      ".\\notMNIST_large\\G\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0945814\n",
      "Standard deviation: 0.44624\n",
      "Pickling .\\notMNIST_large\\H.pickle.\n",
      ".\\notMNIST_large\\H\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: -0.0685221\n",
      "Standard deviation: 0.454232\n",
      "Pickling .\\notMNIST_large\\I.pickle.\n",
      ".\\notMNIST_large\\I\n",
      "Full dataset tensor: (52912, 28, 28)\n",
      "Mean: 0.0307862\n",
      "Standard deviation: 0.468899\n",
      "Pickling .\\notMNIST_large\\J.pickle.\n",
      ".\\notMNIST_large\\J\n",
      "Full dataset tensor: (52911, 28, 28)\n",
      "Mean: -0.153358\n",
      "Standard deviation: 0.443656\n",
      "Pickling .\\notMNIST_small\\A.pickle.\n",
      ".\\notMNIST_small\\A\n",
      "Could not read: .\\notMNIST_small\\A\\RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png : cannot identify image file '.\\\\notMNIST_small\\\\A\\\\RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.132626\n",
      "Standard deviation: 0.445128\n",
      "Pickling .\\notMNIST_small\\B.pickle.\n",
      ".\\notMNIST_small\\B\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: 0.00535609\n",
      "Standard deviation: 0.457115\n",
      "Pickling .\\notMNIST_small\\C.pickle.\n",
      ".\\notMNIST_small\\C\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.141521\n",
      "Standard deviation: 0.44269\n",
      "Pickling .\\notMNIST_small\\D.pickle.\n",
      ".\\notMNIST_small\\D\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0492167\n",
      "Standard deviation: 0.459759\n",
      "Pickling .\\notMNIST_small\\E.pickle.\n",
      ".\\notMNIST_small\\E\n",
      "Full dataset tensor: (1873, 28, 28)\n",
      "Mean: -0.0599148\n",
      "Standard deviation: 0.45735\n",
      "Pickling .\\notMNIST_small\\F.pickle.\n",
      ".\\notMNIST_small\\F\n",
      "Could not read: .\\notMNIST_small\\F\\Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png : cannot identify image file '.\\\\notMNIST_small\\\\F\\\\Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png' - it's ok, skipping.\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.118185\n",
      "Standard deviation: 0.452279\n",
      "Pickling .\\notMNIST_small\\G.pickle.\n",
      ".\\notMNIST_small\\G\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0925503\n",
      "Standard deviation: 0.449006\n",
      "Pickling .\\notMNIST_small\\H.pickle.\n",
      ".\\notMNIST_small\\H\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.0586893\n",
      "Standard deviation: 0.458759\n",
      "Pickling .\\notMNIST_small\\I.pickle.\n",
      ".\\notMNIST_small\\I\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: 0.0526451\n",
      "Standard deviation: 0.471894\n",
      "Pickling .\\notMNIST_small\\J.pickle.\n",
      ".\\notMNIST_small\\J\n",
      "Full dataset tensor: (1872, 28, 28)\n",
      "Mean: -0.151689\n",
      "Standard deviation: 0.448014\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQFCEHwiK8ANBEX4gKMIPBPX/TwgHEm8h6YMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb8eec88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(train_datasets[1], 'rb') as f:\n",
    "    letter_set = pickle.load(f)\n",
    "    plt.imshow(letter_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\notMNIST_large\\A.pickle :  52909\n",
      ".\\notMNIST_large\\B.pickle :  52911\n",
      ".\\notMNIST_large\\C.pickle :  52912\n",
      ".\\notMNIST_large\\D.pickle :  52911\n",
      ".\\notMNIST_large\\E.pickle :  52912\n",
      ".\\notMNIST_large\\F.pickle :  52912\n",
      ".\\notMNIST_large\\G.pickle :  52912\n",
      ".\\notMNIST_large\\H.pickle :  52912\n",
      ".\\notMNIST_large\\I.pickle :  52912\n",
      ".\\notMNIST_large\\J.pickle :  52911\n",
      ".\\notMNIST_small\\A.pickle :  1872\n",
      ".\\notMNIST_small\\B.pickle :  1873\n",
      ".\\notMNIST_small\\C.pickle :  1873\n",
      ".\\notMNIST_small\\D.pickle :  1873\n",
      ".\\notMNIST_small\\E.pickle :  1873\n",
      ".\\notMNIST_small\\F.pickle :  1872\n",
      ".\\notMNIST_small\\G.pickle :  1872\n",
      ".\\notMNIST_small\\H.pickle :  1872\n",
      ".\\notMNIST_small\\I.pickle :  1872\n",
      ".\\notMNIST_small\\J.pickle :  1872\n"
     ]
    }
   ],
   "source": [
    "for pickle_file_train in train_datasets:\n",
    "    # encoding needs to be added beacuse pickle file was creted in python2\n",
    "    letter_set_train = pickle.load(open(pickle_file_train, 'rb'), encoding='latin1')\n",
    "    print(pickle_file_train, \": \", len(letter_set_train))\n",
    "for pickle_file_test in test_datasets:\n",
    "    # encoding needs to be added beacuse pickle file was creted in python2\n",
    "    letter_set_test = pickle.load(open(pickle_file_test, 'rb'), encoding='latin1')\n",
    "    print(pickle_file_test, \": \", len(letter_set_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACeCAYAAAAiy/EDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhAQCDTohhATC/wNabeddHI3pPAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19234518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "for i in np.random.randint(0, 1000, 10):\n",
    "    clear_output(wait=\"Ture\")\n",
    "    plt.imshow(np.hstack((train_dataset[i, :, :], test_dataset[i, :, :], valid_dataset[i, :, :])), cmap = 'gray')\n",
    "    plt.title(\"Train Set \" + str(train_labels[i]) + \n",
    "              \"  -  Test Set\" + str(test_labels[i]) + \n",
    "              \"  -  Validation Set\" + str(valid_labels[i]))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 690800506\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparisons to make for this class: 200.0 million\n",
      "\n",
      "The number of matches found in class 0 is 0\n",
      "Comparisons to make for this class: 200.0 million\n",
      "\n",
      "The number of matches found in class 1 is 0\n",
      "Comparisons to make for this class: 200.0 million\n",
      "\n",
      "The number of matches found in class 2 is 0\n",
      "Comparisons to make for this class: 200.0 million\n",
      "\n",
      "The number of matches found in class 3 is 0\n",
      "Comparisons to make for this class: 200.0 million\n",
      "\n",
      "The number of matches found in class 4 is 0\n",
      "Comparisons to make for this class: 200.0 million\n",
      "\n",
      "The number of matches found in class 5 is 0\n",
      "Comparisons to make for this class: 200.0 million\n",
      "\n",
      "The number of matches found in class 6 is 0\n",
      "Comparisons to make for this class: 200.0 million\n",
      "\n",
      "The number of matches found in class 7 is 0\n",
      "Comparisons to make for this class: 200.0 million\n",
      "\n",
      "The number of matches found in class 8 is 0\n",
      "Comparisons to make for this class: 200.0 million\n",
      "\n",
      "The number of matches found in class 9 is 0\n",
      "Total of matches: 0 from a sample size: 200000 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def are_equal(img1, img2):\n",
    "    row = img1.shape[0] / 2\n",
    "    if np.array_equal(img1[row, :], img2[row, :]):\n",
    "        return np.array_equal(img1, img2)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def find_duplicates(labels1, dataset1, labels2=None, dataset2=None):\n",
    "    classes = np.unique(labels1)\n",
    "    duplicate_index = []\n",
    "    try:\n",
    "        if not labels2:\n",
    "            for _class in classes:\n",
    "                num_duplicates = 0\n",
    "                index = np.where(labels1 == _class)[0]\n",
    "                combinations = list(it.combinations(range(len(index)), 2))\n",
    "                print(\"Comparisons to make for this class:\", len(combinations) / float(1000000), \"million\")\n",
    "                for i,j in combinations:\n",
    "                    img1 = dataset1[i]\n",
    "                    img2 = dataset1[j]\n",
    "                    if are_equal(img1, img2) == True:\n",
    "                        duplicate_index.append((i ,j))\n",
    "                        num_duplicates += 1\n",
    "                print(\"The number of matches found in class %d is %d\\n\" %(_class, num_duplicates))\n",
    "            print(\"Total of matches: %d from a sample size: %d \\n\\n\" %(len(duplicate_index), len(labels1)))\n",
    "    \n",
    "    except:\n",
    "        for _class in classes:\n",
    "            num_duplicates = 0\n",
    "            index1 = np.where(labels1 == _class)[0]\n",
    "            index2 = np.where(labels2 == _class)[0]\n",
    "            print(\"Comparisons to make for this class:\", len(index1) * len(index1) / float(1000000) / 2 , \"million\\n\")\n",
    "            for i in index1:\n",
    "                for j in index2:\n",
    "                    if j >= i:\n",
    "                        img1 = dataset1[i]\n",
    "                        img2 = dataset2[j]\n",
    "                        if are_equal(img1, img2) == True:\n",
    "                            duplicate_index.append((i ,j))\n",
    "                            num_duplicates += 1\n",
    "            print(\"The number of matches found in class %d is %d\" %(_class, num_duplicates))\n",
    "        print(\"Total of matches: %d from a sample size: %d \\n\\n\" %(len(duplicate_index), len(labels1)))\n",
    "    return duplicate_index\n",
    "         \n",
    "# duplicate_index_valid = find_duplicates(valid_labels, valid_dataset)\n",
    "# duplicate_index_test = find_duplicates(test_labels, test_dataset)\n",
    "# duplicate_index_train = find_duplicates(train_labels, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples in train dataset: 200.0 thousand\n",
      "Examples in valid dataset: 10.0 thousand\n",
      "Examples in test dataset: 10.0 thousand\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "dic_file = pickle.load(open(pickle_file, 'rb'))\n",
    "train_dataset = dic_file[\"train_dataset\"]\n",
    "train_labels = dic_file[\"train_labels\"]\n",
    "valid_dataset = dic_file[\"valid_dataset\"]\n",
    "valid_labels = dic_file[\"valid_labels\"]\n",
    "test_dataset = dic_file[\"test_dataset\"]\n",
    "test_labels = dic_file[\"test_labels\"]\n",
    "\n",
    "print(\"Examples in train dataset:\", len(train_labels) / float(1000), \"thousand\")\n",
    "print(\"Examples in valid dataset:\", len(valid_labels) / float(1000), \"thousand\")\n",
    "print(\"Examples in test dataset:\", len(test_labels) / float(1000), \"thousand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score\n",
      "\n",
      "Wall time: 5.66 s\n",
      "Samples: 100 ---> Score: 0.609903316165\n",
      "Wall time: 19.8 s\n",
      "Samples: 1000 ---> Score: 0.757924555364\n",
      "Wall time: 3min 50s\n",
      "Samples: 5000 ---> Score: 0.805396609358\n",
      "Wall time: 17min 28s\n",
      "Samples: 50000 ---> Score: 0.809906313499\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def train_and_validate(num_examples):\n",
    "    # flatten datasets\n",
    "    flat_train = [x.flatten() for x in train_dataset[:num_examples]]\n",
    "    flat_valid = [x.flatten() for x in valid_dataset[:num_examples]]\n",
    "#     print(train_labels[:num_examples]) # Check if labels are really shuffled\n",
    "    model = LogisticRegressionCV().fit(flat_train, train_labels[:num_examples])\n",
    "    score = cross_val_score(model, flat_valid, valid_labels[:num_examples]).mean()\n",
    "    return score\n",
    "\n",
    "training_sizes = [100, 1000, 5000, 50000]\n",
    "print(\"Cross Validation Score\\n\")\n",
    "for size in training_sizes:\n",
    "    %time score = train_and_validate(size)\n",
    "    print(\"Samples:\", size,\"---> Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
